{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will try to train different classification models on the data like Logistic Regression, k-nearest neighbors classifier, Random Forest and a Support Vector Machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m#Import required libraries\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpandas\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mpd\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpyplot\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mplt\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "#Import required libraries\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'c:\\Users\\Shubham Rathi\\AppData\\Local\\Programs\\Python\\Python311\\python.exe' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '\"c:/Users/Shubham Rathi/AppData/Local/Programs/Python/Python311/python.exe\" -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "#File does not contain headers so we need to load the headers manually\n",
    "features = [\"Age\", \"Gender\", \"Total Bilirubin\", \"Direct Bilirubin\", \"Alkphos Alkaline Phosphotase\", \"Sgpt Alamine Aminotransferase\", \"Sgot Aspartate Aminotransferase\", \"Total Protiens\", \"Albumin\", \"Albumin-Globulin Ratio\", \"Selector\"]\n",
    "data = pd.read_csv('ilpd.csv', names = features)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'c:\\Users\\Shubham Rathi\\AppData\\Local\\Programs\\Python\\Python311\\python.exe' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '\"c:/Users/Shubham Rathi/AppData/Local/Programs/Python/Python311/python.exe\" -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "#Overview of data\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Albumin-Globulin Ratio feature has four missing values, as seen above. Here, we are dropping those particular rows which have missing data. We could, in fact, fill those place with values of our own, using options like: \n",
    "1. A constant value that has meaning within the domain, such as 0, distinct from all other values.\n",
    "2. A value from another randomly selected record, or the immediately next or previous record.\n",
    "3. A mean, median or mode value for the column.\n",
    "4. A value estimated by another predictive model.\n",
    "\n",
    "But here, since a very small fraction of values are missing, we choose to drop those rows. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'c:\\Users\\Shubham Rathi\\AppData\\Local\\Programs\\Python\\Python311\\python.exe' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '\"c:/Users/Shubham Rathi/AppData/Local/Programs/Python/Python311/python.exe\" -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "#Transfrom Gender string into float values\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(['Male','Female'])\n",
    "data.loc[:,'Gender'] = le.transform(data['Gender'])\n",
    "\n",
    "#Remove rows with missing values\n",
    "data = data.dropna(how = 'any', axis = 0)\n",
    "\n",
    "#Also transform Selector variable into usual conventions followed\n",
    "data['Selector'] = data['Selector'].map({2:0, 1:1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'c:\\Users\\Shubham Rathi\\AppData\\Local\\Programs\\Python\\Python311\\python.exe' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '\"c:/Users/Shubham Rathi/AppData/Local/Programs/Python/Python311/python.exe\" -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "#Overview of data\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'c:\\Users\\Shubham Rathi\\AppData\\Local\\Programs\\Python\\Python311\\python.exe' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '\"c:/Users/Shubham Rathi/AppData/Local/Programs/Python/Python311/python.exe\" -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "#features characteristics to determine if feature scaling is necessary\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It clearly reflects from the above output that values across different variables are distributed far too extensively. We could perform feature scaling or normalization so as to improve the classifier accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'c:\\Users\\Shubham Rathi\\AppData\\Local\\Programs\\Python\\Python311\\python.exe' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '\"c:/Users/Shubham Rathi/AppData/Local/Programs/Python/Python311/python.exe\" -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data, data['Selector'], random_state = 0)\n",
    "scaler = MinMaxScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'c:\\Users\\Shubham Rathi\\AppData\\Local\\Programs\\Python\\Python311\\python.exe' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '\"c:/Users/Shubham Rathi/AppData/Local/Programs/Python/Python311/python.exe\" -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "#Determining the healthy-affected split\n",
    "print(\"Positive records:\", data['Selector'].value_counts().iloc[0])\n",
    "print(\"Negative records:\", data['Selector'].value_counts().iloc[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This output confirms that we have 414 positive and 165 negative records. This indicates that this is a highly unbalanced dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m#Determine statistics based on age\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m plt\u001b[39m.\u001b[39mfigure(figsize\u001b[39m=\u001b[39m(\u001b[39m12\u001b[39m, \u001b[39m10\u001b[39m))\n\u001b[0;32m      3\u001b[0m plt\u001b[39m.\u001b[39mhist(data[data[\u001b[39m'\u001b[39m\u001b[39mSelector\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mAge\u001b[39m\u001b[39m'\u001b[39m], bins \u001b[39m=\u001b[39m \u001b[39m16\u001b[39m, align \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mmid\u001b[39m\u001b[39m'\u001b[39m, rwidth \u001b[39m=\u001b[39m \u001b[39m0.5\u001b[39m, color \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mblack\u001b[39m\u001b[39m'\u001b[39m, alpha \u001b[39m=\u001b[39m \u001b[39m0.8\u001b[39m)\n\u001b[0;32m      4\u001b[0m plt\u001b[39m.\u001b[39mxlabel(\u001b[39m'\u001b[39m\u001b[39mAge\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "#Determine statistics based on age\n",
    "plt.figure(figsize=(12, 10))\n",
    "plt.hist(data[data['Selector'] == 1]['Age'], bins = 16, align = 'mid', rwidth = 0.5, color = 'black', alpha = 0.8)\n",
    "plt.xlabel('Age')\n",
    "plt.ylabel('Number of Patients')\n",
    "plt.title('Frequency-Age Distribution')\n",
    "plt.grid(True)\n",
    "plt.savefig('fig1')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the age vs. frequency graph, we can observe that middle-aged people are the worst affected. Even elderly people are also suffering from liver ailments, as seen by the bar sizes at ages 60-80."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'c:\\Users\\Shubham Rathi\\AppData\\Local\\Programs\\Python\\Python311\\python.exe' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '\"c:/Users/Shubham Rathi/AppData/Local/Programs/Python/Python311/python.exe\" -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "#correlation-matrix\n",
    "plt.subplots(figsize=(12, 10))\n",
    "plt.title('Pearson Correlation of Features')\n",
    "# Draw the heatmap using seaborn\n",
    "sns.heatmap(data.corr(),linewidths=0.25, vmax=1.0, square=True,annot=True)\n",
    "plt.savefig('fig2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The correlation matrix gives us the relationship between two features. As seen above, the following pairs of features seem to be very closely related as indicated by their high correlation coefficients:\n",
    "1. Total Bilirubin and Direct Bilirubin(0.87)\n",
    "2. Sgpt Alamine Aminotransferase and Sgot Aspartate Aminotransferase(0.79)\n",
    "3. Albumin and Total Proteins(0.78)\n",
    "4. Albumin and Albumin-Globulin Ratio(0.69)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Using Classification Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now evaluate the performance of various classifiers on this dataset. For the sake of understanding as to how feature scaling affects classifier performance, we will train models using both scaled and unscaled data.\n",
    "Since we are interested in capturing records of people who have been tested positive, we will base our classifier evaluation metric on precision and recall instead of accuracy.\n",
    "We could also use F1 score, since it takes into account both precision and recall."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Logistic Regression Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'c:\\Users\\Shubham Rathi\\AppData\\Local\\Programs\\Python\\Python311\\python.exe' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '\"c:/Users/Shubham Rathi/AppData/Local/Programs/Python/Python311/python.exe\" -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "#Using normal data\n",
    "logreg = LogisticRegression(C = 0.1).fit(X_train, y_train)\n",
    "print(\"Logistic Regression Classifier on unscaled test data:\")\n",
    "print(\"Accuracy:\", logreg.score(X_test, y_test))\n",
    "print(\"Precision:\", precision_score(y_test, logreg.predict(X_test)))\n",
    "print(\"Recall:\", recall_score(y_test, logreg.predict(X_test)))\n",
    "print(\"F-1 score:\", f1_score(y_test, logreg.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'c:\\Users\\Shubham Rathi\\AppData\\Local\\Programs\\Python\\Python311\\python.exe' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '\"c:/Users/Shubham Rathi/AppData/Local/Programs/Python/Python311/python.exe\" -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "#Using feature-scaled data\n",
    "logreg_scaled = LogisticRegression(C = 0.1).fit(X_train_scaled, y_train)\n",
    "print(\"Logistic Regression Classifier on scaled test data:\")\n",
    "print(\"Accuracy:\", logreg_scaled.score(X_test_scaled, y_test))\n",
    "print(\"Precision:\", precision_score(y_test, logreg_scaled.predict(X_test_scaled)))\n",
    "print(\"Recall:\", recall_score(y_test, logreg_scaled.predict(X_test_scaled)))\n",
    "print(\"F-1 score:\", f1_score(y_test, logreg_scaled.predict(X_test_scaled)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well! The performance has definitely improved by feature scaling, though not drastically, as there was already very little scope of improvement.\n",
    "Let us look at other classifiers and analyse how they react to scaling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. SVM Classifier with RBF kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'c:\\Users\\Shubham Rathi\\AppData\\Local\\Programs\\Python\\Python311\\python.exe' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '\"c:/Users/Shubham Rathi/AppData/Local/Programs/Python/Python311/python.exe\" -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "#Using normal data\n",
    "svc_clf = SVC(C = 0.1, kernel = 'rbf').fit(X_train, y_train)\n",
    "print(\"SVM Classifier on unscaled test data:\")\n",
    "print(\"Accuracy:\", svc_clf.score(X_test, y_test))\n",
    "print(\"Precision:\", precision_score(y_test, svc_clf.predict(X_test)))\n",
    "print(\"Recall:\", recall_score(y_test, svc_clf.predict(X_test)))\n",
    "print(\"F-1 score:\", f1_score(y_test, svc_clf.predict(X_test)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'c:\\Users\\Shubham Rathi\\AppData\\Local\\Programs\\Python\\Python311\\python.exe' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '\"c:/Users/Shubham Rathi/AppData/Local/Programs/Python/Python311/python.exe\" -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "#Using scaled data\n",
    "svc_clf_scaled = SVC(C = 0.1, kernel = 'rbf').fit(X_train_scaled, y_train)\n",
    "print(\"SVM Classifier on scaled test data:\")\n",
    "print(\"Accuracy:\", svc_clf_scaled.score(X_test_scaled, y_test))\n",
    "print(\"Precision:\", precision_score(y_test, svc_clf_scaled.predict(X_test_scaled)))\n",
    "print(\"Recall:\", recall_score(y_test, svc_clf_scaled.predict(X_test_scaled)))\n",
    "print(\"F-1 score:\", f1_score(y_test, svc_clf_scaled.predict(X_test_scaled)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again perfect scores! This time, the performance improvement is quite noticeable, as there is a rise of almost 0.18 in the F-1 score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. K-NN Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'c:\\Users\\Shubham Rathi\\AppData\\Local\\Programs\\Python\\Python311\\python.exe' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '\"c:/Users/Shubham Rathi/AppData/Local/Programs/Python/Python311/python.exe\" -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "#Using normal data\n",
    "knn = KNeighborsClassifier(n_neighbors = 5)\n",
    "knn.fit(X_train, y_train)\n",
    "print(\"k-NN Classifier on unscaled test data:\")\n",
    "print(\"Accuracy:\", knn.score(X_test, y_test))\n",
    "print(\"Precision:\", precision_score(y_test, knn.predict(X_test)))\n",
    "print(\"Recall:\", recall_score(y_test, knn.predict(X_test)))\n",
    "print(\"F-1 score:\", f1_score(y_test, knn.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'c:\\Users\\Shubham Rathi\\AppData\\Local\\Programs\\Python\\Python311\\python.exe' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '\"c:/Users/Shubham Rathi/AppData/Local/Programs/Python/Python311/python.exe\" -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "#Using scaled data\n",
    "knn_scaled = KNeighborsClassifier(n_neighbors = 5)\n",
    "knn_scaled.fit(X_train_scaled, y_train)\n",
    "print(\"SVM Classifier on scaled test data:\")\n",
    "print(\"Accuracy:\", knn_scaled.score(X_test_scaled, y_test))\n",
    "print(\"Precision:\", precision_score(y_test, knn_scaled.predict(X_test_scaled)))\n",
    "print(\"Recall:\", recall_score(y_test, knn_scaled.predict(X_test_scaled)))\n",
    "print(\"F-1 score:\", f1_score(y_test, knn_scaled.predict(X_test_scaled)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'c:\\Users\\Shubham Rathi\\AppData\\Local\\Programs\\Python\\Python311\\python.exe' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '\"c:/Users/Shubham Rathi/AppData/Local/Programs/Python/Python311/python.exe\" -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "#using normal data\n",
    "rfc = RandomForestClassifier(n_estimators = 20)\n",
    "rfc.fit(X_train, y_train)\n",
    "print(\"SVM Classifier on unscaled test data:\")\n",
    "print(\"Accuracy:\", rfc.score(X_test, y_test))\n",
    "print(\"Precision:\", precision_score(y_test, rfc.predict(X_test)))\n",
    "print(\"Recall:\", recall_score(y_test, rfc.predict(X_test)))\n",
    "print(\"F-1 score:\", f1_score(y_test, rfc.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'c:\\Users\\Shubham Rathi\\AppData\\Local\\Programs\\Python\\Python311\\python.exe' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '\"c:/Users/Shubham Rathi/AppData/Local/Programs/Python/Python311/python.exe\" -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "#using scaled data\n",
    "rfc_scaled = RandomForestClassifier(n_estimators = 20)\n",
    "rfc_scaled.fit(X_train_scaled, y_train)\n",
    "print(\"Random Forest Classifier on scaled test data:\")\n",
    "print(\"Accuracy:\", rfc_scaled.score(X_test_scaled, y_test))\n",
    "print(\"Precision:\", precision_score(y_test, rfc_scaled.predict(X_test_scaled)))\n",
    "print(\"Recall:\", recall_score(y_test, rfc_scaled.predict(X_test_scaled)))\n",
    "print(\"F-1 score:\", f1_score(y_test, rfc_scaled.predict(X_test_scaled)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest Classifier works well either way, it seems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature scaling definitely was helpful in improving model performance. While the Random Forest Classifier performed equally well with and without feature scaling, it is not necessary that this may be the case always. For, performance issues with feature scaling depend on a lot of factors:\n",
    "1. Understanding the data distribution is important. It is highly possible that there may be some features which are almost constant except for a small noise-driven variation. This noise would then be amplified greatly by the normalization.\n",
    "2. The regularization parameter C is also a very important factor in classifier performance. This is crucial either way, whether or not feature scaling is done."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Further applications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is not expected to get the same level of performance on bigger and denser data sets. The main reasons behind this are:\n",
    "1. The dataset we worked on was very small, consisting of only 583 observations, of which 4 were dropped.\n",
    "2. The dataset was highly unbalanced, the postive records being three times the number of negative ones.\n",
    "\n",
    "Hence, even though we have obtained perfect scores on this dataset, the performance of the same models on similar but bigger datasets is expected to worsen."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
